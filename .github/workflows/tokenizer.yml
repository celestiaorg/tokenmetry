name: Token Telemetry

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch: # Allow manual triggering
  push:
    branches: [ main ]
    paths:
      - 'tokenizer.py'
      - '.github/workflows/tokenizer.yml'
      - 'repos.txt'

permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  analyze-tokens:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
        
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v4
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
        
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-interaction --no-root
      
    - name: Run token analysis and prepare for deployment
      run: |
        # Create deployment directory
        mkdir -p _site
        
        # Run token analysis and output directly to deployment directory
        poetry run python tokenizer.py --celestia-repos --output _site/index.json
        
        # Add metadata for AI agents
        python3 << 'EOF'
        import json
        from datetime import datetime, timezone
        from collections import OrderedDict
        
        # Load existing results
        with open('_site/index.json', 'r') as f:
            data = json.load(f)
        
        # Create new ordered structure with metadata first
        new_data = OrderedDict()
        
        # Add comprehensive metadata for AI agents at the top
        new_data['metadata'] = {
            'generated_at': datetime.now(timezone.utc).isoformat(),
            'generator': 'celestiaorg/tokenmetry',
            'version': '1.0.0',
            'format': 'celestia-token-telemetry-v1',
            'description': 'Token count analysis of CelestiaOrg repositories for AI development and context management',
            'api_endpoint': 'https://celestiaorg.github.io/tokenmetry/index.json',
            'purpose': 'This JSON provides comprehensive token analysis of CelestiaOrg repositories to help AI agents understand codebase size, complexity, and structure for better context management and development assistance.',
            'usage_instructions': {
                'overview': 'Use summary.total_tokens for overall codebase size. Check individual repositories for specific analysis.',
                'repository_discovery': 'To see which repositories are analyzed, iterate through the repositories array and check each repository.name and repository.url field.',
                'context_management': 'Use token counts to determine how much of a repository can fit in your context window. Average tokens per file can help estimate partial loading strategies.',
                'repository_selection': 'Compare repositories by total_tokens to prioritize which ones to analyze first based on your context limits.',
                'file_analysis': 'Use the files array in each repository to identify the largest files that might need special handling or chunking.',
                'language_breakdown': 'Use by_extension data to understand the mix of Go code vs documentation in each repository.'
            },
            'data_structure': {
                'summary': 'High-level aggregated statistics across all repositories',
                'repositories': 'Array of individual repository analyses with detailed file breakdowns',
                'token_counting': 'Uses GPT-2 tokenizer for consistent token counting across all content'
            }
        }
        
        # Add all other data
        for key, value in data.items():
            if key != 'metadata':  # Don't duplicate metadata
                new_data[key] = value
        
        # Save updated results with metadata first
        with open('_site/index.json', 'w') as f:
            json.dump(new_data, f, indent=2)
        EOF
        
    - name: Setup Pages
      uses: actions/configure-pages@v4
      
    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: './_site'
        
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: analyze-tokens
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
