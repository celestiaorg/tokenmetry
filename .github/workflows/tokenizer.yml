name: Token Telemetry

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch: # Allow manual triggering
  push:
    branches: [ main ]
    paths:
      - 'tokenizer.py'
      - '.github/workflows/tokenizer.yml'
      - 'repos.txt'

permissions:
  contents: write
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  analyze-tokens:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
        
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v4
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
        
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-interaction --no-root
      
    - name: Run token analysis
      run: |
        mkdir -p _site
        poetry run python tokenizer.py --celestia-repos --output _site/index.json
        
    - name: Create index.html redirect
      run: |
        cat > _site/index.html << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>Token Telemetry API</title>
            <meta http-equiv="refresh" content="0; url=index.json">
        </head>
        <body>
            <h1>ðŸš€ Token Telemetry API</h1>
            <p>Comprehensive token analysis of CelestiaOrg repositories for AI development and context management.</p>
            <p>Redirecting to <a href="index.json">index.json</a>...</p>
            <p><small>Direct link: <code>https://celestiaorg.github.io/tokenmetry/index.json</code></small></p>
            <p><a href="https://github.com/celestiaorg/tokenmetry">ðŸ“š View Documentation on GitHub</a></p>
        </body>
        </html>
        EOF
        
    - name: Add metadata to JSON
      run: |
        python3 -c "
        import json
        from datetime import datetime, timezone
        from collections import OrderedDict
        from transformers import GPT2TokenizerFast
        
        with open('_site/index.json', 'r') as f:
            data = json.load(f)
        
        new_data = OrderedDict()
        new_data['metadata'] = {
            'generated_at': datetime.now(timezone.utc).isoformat(),
            'generator': 'celestiaorg/tokenmetry',
            'version': '1.0.0',
            'format': 'celestia-token-telemetry-v1',
            'description': 'Token count analysis of CelestiaOrg repositories for AI development and context management',
            'api_endpoint': 'https://celestiaorg.github.io/tokenmetry/index.json',
            'purpose': 'This JSON provides comprehensive token analysis of CelestiaOrg repositories to help AI agents understand codebase size, complexity, and structure for better context management and development assistance.',
            'usage_instructions': {
                'overview': 'Use summary.total_tokens for overall codebase size. Check individual repositories for specific analysis.',
                'repository_discovery': 'To see which repositories are analyzed, iterate through the repositories array and check each repository.name and repository.url field.',
                'context_management': 'Use token counts to determine how much of a repository can fit in your context window. Average tokens per file can help estimate partial loading strategies.',
                'repository_selection': 'Compare repositories by total_tokens to prioritize which ones to analyze first based on your context limits.',
                'file_analysis': 'Use the files array in each repository to identify the largest files that might need special handling or chunking.',
                'language_breakdown': 'Use by_extension data to understand the mix of Go code vs documentation in each repository.',
                'context_window_warning': {
                    'message': 'CAUTION: This JSON file itself can be large and may consume significant tokens in your context window.',
                    'recommendation': 'DO NOT attempt to load or process this entire file at once if your context is constrained.',
                    'retrieval_strategy': [
                        '1. Start by fetching and analyzing only the metadata and summary sections for a high-level overview.',
                        '2. To analyze specific repositories: Iterate through the repositories array. Process the data for ONE repository at a time.',
                        '3. For very large repositories: First, examine the repository-level summary (e.g., total_tokens, total_files, by_extension). Then, if necessary and context allows, selectively process its files array, potentially in chunks.',
                        '4. Consider using streaming JSON parsers or selective field extraction rather than loading the complete JSON into memory.'
                    ]
                }
            },
            'data_structure': {
                'summary': 'High-level aggregated statistics across all repositories',
                'repositories': 'Array of individual repository analyses with detailed file breakdowns',
                'token_counting': 'Uses GPT-2 tokenizer for consistent token counting across all content'
            }
        }
        
        for key, value in data.items():
            if key != 'metadata':
                new_data[key] = value
        
        # Calculate approximate token count of this JSON file itself
        json_content = json.dumps(new_data, indent=2)
        try:
            tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
            json_token_count = len(tokenizer.encode(json_content))
            new_data['metadata']['usage_instructions']['context_window_warning']['json_file_token_count'] = json_token_count
            new_data['metadata']['usage_instructions']['context_window_warning']['message'] = f'CAUTION: This JSON file itself contains approximately {json_token_count:,} tokens and may consume significant space in your context window.'
        except Exception as e:
            # Fallback to character-based estimation if tokenizer fails
            char_count = len(json_content)
            estimated_tokens = char_count // 4  # Rough estimate: ~4 chars per token
            new_data['metadata']['usage_instructions']['context_window_warning']['json_file_token_count'] = estimated_tokens
            new_data['metadata']['usage_instructions']['context_window_warning']['message'] = f'CAUTION: This JSON file itself contains approximately {estimated_tokens:,} tokens (estimated) and may consume significant space in your context window.'
        
        with open('_site/index.json', 'w') as f:
            json.dump(new_data, f, indent=2)
        "
        
    - name: Update README table
      run: |
        python3 -c "
        import json
        import re
        from datetime import datetime, timezone
        
        with open('_site/index.json', 'r') as f:
            data = json.load(f)
        
        table_rows = []
        table_rows.append('| Repository | Total Files | Total Tokens | Go Files | Go Tokens | Markdown Files | Markdown Tokens |')
        table_rows.append('|------------|-------------|--------------|----------|-----------|----------------|-----------------|')
        
        for repo in data['repositories']:
            if 'error' not in repo:
                name = repo['repository']['name']
                total_files = repo['total_files']
                total_tokens = f\"{repo['total_tokens']:,}\"
                
                go_files = repo['by_extension'].get('.go', {}).get('files', 0)
                go_tokens = f\"{repo['by_extension'].get('.go', {}).get('tokens', 0):,}\"
                
                md_files = repo['by_extension'].get('.md', {}).get('files', 0)
                md_tokens = f\"{repo['by_extension'].get('.md', {}).get('tokens', 0):,}\"
                
                table_rows.append(f'| {name} | {total_files} | {total_tokens} | {go_files} | {go_tokens} | {md_files} | {md_tokens} |')
        
        summary = data['summary']
        table_rows.append(f'| **TOTAL** | **{summary[\"total_files\"]}** | **{summary[\"total_tokens\"]:,}** | **{summary[\"by_extension\"][\".go\"][\"files\"]}** | **{summary[\"by_extension\"][\".go\"][\"tokens\"]:,}** | **{summary[\"by_extension\"][\".md\"][\"files\"]}** | **{summary[\"by_extension\"][\".md\"][\"tokens\"]:,}** |')
        
        table_content = '\n'.join(table_rows)
        timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
        table_with_timestamp = f'{table_content}\n\n**Last Updated:** {timestamp}'
        
        with open('README.md', 'r') as f:
            readme_content = f.read()
        
        pattern = r'<!-- TOKENMETRY_TABLE_START -->.*?<!-- TOKENMETRY_TABLE_END -->'
        replacement = f'<!-- TOKENMETRY_TABLE_START -->\n{table_with_timestamp}\n<!-- TOKENMETRY_TABLE_END -->'
        
        updated_readme = re.sub(pattern, replacement, readme_content, flags=re.DOTALL)
        
        with open('README.md', 'w') as f:
            f.write(updated_readme)
        "
        
    - name: Commit README updates
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add README.md
        if git diff --staged --quiet; then
          echo "No changes to README.md"
        else
          git commit -m "Update README.md with latest token analysis results [skip ci]"
          git push
        fi
        
    - name: Setup Pages
      uses: actions/configure-pages@v4
      
    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: './_site'
        
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: analyze-tokens
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
